{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38aee0e",
   "metadata": {},
   "source": [
    "originally found here : https://github.com/ImperialNLP/NLPLabs/blob/c724834960345085690802233966682bc3321723/lab06/lab06_solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dccf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3109480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    " os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148895e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ade9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data_test/training_data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lan = 'en'\n",
    "trg_lan = 'fr'\n",
    "# data[src_lan].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b440cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30K:\n",
    "    \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
    "    def __init__(self, tokenizer, src_lan, trg_lan, df):\n",
    "        \n",
    "        #TODO : raise an error of source and target languages are not in the dataframe columns\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_lan = src_lan\n",
    "        self.trg_lan = trg_lan\n",
    "        self.src_sents, self.trg_sents = self.read_sentences(df)\n",
    "\n",
    "    def read_sentences(self, df):\n",
    "        src_sents = data[self.src_lan].tolist()\n",
    "        trg_sents = data[self.trg_lan].tolist()\n",
    "        return src_sents, trg_sents\n",
    "    \n",
    "    def collate_fn(self, idx):\n",
    "        src_texts = [self.src_sents[i] for i in idx]\n",
    "        trg_texts = [self.trg_sents[i] for i in idx]\n",
    "        \n",
    "        output = self.tokenizer.prepare_seq2seq_batch(src_texts=src_texts, \n",
    "                                                      tgt_texts=trg_texts, \n",
    "                                                      max_length=128, \n",
    "                                                      max_target_length=128,\n",
    "                                                      return_tensors='pt',\n",
    "                                                      truncation=True)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2661e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
    "pretrainedModelName = 'Helsinki-NLP/opus-mt-'+ src_lan + '-' + trg_lan\n",
    "print(pretrainedModelName)\n",
    "model = MarianMTModel.from_pretrained(pretrainedModelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = int(0.1 * len(data))\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c83b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = choices(range(len(data)),k=num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.iloc[ids_test]\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.drop(ids_test)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8230ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mt():\n",
    "    \n",
    "    ## QUESTION 5 ##\n",
    "\n",
    "    mt_tokenizer = MarianTokenizer.from_pretrained(pretrainedModelName)\n",
    "    mt_dataset = Multi30K(mt_tokenizer, 'en','fr',data_train)\n",
    "    \n",
    "    model = MarianMTModel.from_pretrained(pretrainedModelName)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./data/experiment/mt',\n",
    "        learning_rate = 0.00005,\n",
    "        logging_steps= 5000,\n",
    "        save_steps = 10000,\n",
    "        num_train_epochs = 1,\n",
    "        per_device_train_batch_size=2\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=mt_dataset,                     \n",
    "        data_collator=mt_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    ## when you already trained your model and want to start from a checkpoint\n",
    "    #trainer.train(\"./experiment/mt/checkpoint-40000\")\n",
    "\n",
    "    trainer.save_model('./data/models/custom' + pretrainedModelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf26d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def evaluate_mt(model,mt_tokenizer, mt_test_dataset):\n",
    "\n",
    "    bleu = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #   for file in tqdm(range(len(mt_test_dataset))):\n",
    "    for file in range(len(mt_test_dataset)):\n",
    "        src_text = mt_test_dataset.src_sents[file]\n",
    "        targ_text_origin = mt_test_dataset.trg_sents[file]\n",
    "\n",
    "        translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
    "        translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "        bleu.append(sacrebleu.corpus_bleu(translated_text, targ_text_origin, force=True).score)\n",
    "\n",
    "    bleu = np.asarray(bleu)\n",
    "\n",
    "    return np.average(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarianMTModel.from_pretrained('./data/models/custom' + pretrainedModelName)\n",
    "\n",
    "mt_tokenizer = MarianTokenizer.from_pretrained(pretrainedModelName)\n",
    "mt_test_dataset = Multi30K(mt_tokenizer, 'en','fr',data_test)\n",
    "\n",
    "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\n",
    "\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0df79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mt(model,mt_tokenizer, mt_test_dataset):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #   for file in tqdm(range(len(mt_test_dataset))):\n",
    "    for file in range(len(mt_test_dataset)):\n",
    "        src_text = mt_test_dataset.src_sents[file]\n",
    "        targ_text_origin = mt_test_dataset.trg_sents[file]\n",
    "\n",
    "        translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
    "        translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "        \n",
    "        print('source : ' + str(src_text))\n",
    "        print('original : ' + str(targ_text_origin))\n",
    "        print('translated : ' + str(translated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c63dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mt(model,mt_tokenizer, mt_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415154e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
