{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21b0bf5",
   "metadata": {},
   "source": [
    "found here : https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32972160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "  )\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d6ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size of data is 1609682\n"
     ]
    }
   ],
   "source": [
    "rootdata = \"../data_test/parallel/\"\n",
    "data = []\n",
    "with open(rootdata+\"IITB.en-hi.en\") as f2, open(rootdata+\"IITB.en-hi.hi\") as f1:\n",
    "    for src, tgt in zip(f1, f2):\n",
    "      data.append(\n",
    "          {\n",
    "              \"translation\": {\n",
    "                  \"hi\": src.strip(),\n",
    "                  \"en\": tgt.strip()\n",
    "              }\n",
    "          }\n",
    "      )\n",
    "print(f'total size of data is {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1a9733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'hi': '‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç',\n",
       "  'en': 'Give your application an accessibility workout'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb58876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab457742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2df3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train, validation\n",
    "split = 0.2\n",
    "train_dataset, eval_dataset = random_split(data, lengths=[int((1-split)*len(data)), int(split*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40681ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining collator functioon for preparing batches on the fly ..\n",
    "def data_collator(features:list):\n",
    "    labels = [f[\"translation\"][\"en\"] for f in features]\n",
    "    inputs = [f[\"translation\"][\"hi\"] for f in features]\n",
    "\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, src_lang=\"hi_IN\", tgt_lang=\"en_XX\", tgt_texts=labels, max_length=32, max_target_length=32)\n",
    "\n",
    "    for k in batch:\n",
    "        batch[k] = torch.tensor(batch[k])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8822b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2cb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating model, tokenizer\n",
    "# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "# tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33d62b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MarianMTModel.from_pretrained(model_name)\n",
    "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "# sample_text = \"‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç\"\n",
    "sample_text = \"Give your application an accessibility workout\"\n",
    "batch = tokenizer([sample_text], return_tensors=\"pt\")\n",
    "gen = model.generate(**batch)\n",
    "tokenizer.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0ca4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"indic-mbart\",\n",
    "                        do_train=True,\n",
    "                        do_eval=True,\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=16,\n",
    "                        per_device_eval_batch_size=16,\n",
    "                        learning_rate=5e-5,\n",
    "                        num_train_epochs=2,\n",
    "                        logging_dir=\"/home/abarthe/.tensorboard_files/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d11179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining trainer using ü§ó\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                data_collator=data_collator, \n",
    "                train_dataset=train_dataset, \n",
    "                eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9694041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.782735</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>314.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.293343</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>312.617000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=4.564515686035156, metrics={'train_runtime': 1.7641, 'train_samples_per_second': 5.669, 'total_flos': 2287769223168.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 46096, 'init_mem_gpu_alloc_delta': 305772544, 'init_mem_cpu_peaked_delta': 18258, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 209758, 'train_mem_gpu_alloc_delta': 910295040, 'train_mem_cpu_peaked_delta': 45516, 'train_mem_gpu_peaked_delta': 776058368})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac1b125",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dfa68fe14f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Give your application an accessibility workout\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# add encoder_outputs to model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;31m# set input_ids as decoder_input_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, input_ids, model_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0margument\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoder_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             }\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/miniconda3/envs/nmt/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "sample_text = \"Give your application an accessibility workout\"\n",
    "batch = tokenizer([sample_text], return_tensors=\"pt\")\n",
    "gen = model.generate(**batch)\n",
    "tokenizer.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e086a1",
   "metadata": {},
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ò‡§ü‡§®‡§æ ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•ã\"\n",
    "inputs_tokenized = tokenizer(inputs, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = \"finetuned/test\"\n",
    "trainer.save_model(ft_model)\n",
    "tokenizer.save_pretrained(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# model_id = \"vasudevgupta/mbart-iitb-hin-eng\"\n",
    "# translator = pipeline(\"translation_hi_to_en\", model=model_id, tokenizer=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = trainer.model\n",
    "tokenizer_trained = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trained.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = model_trained.generate(**tokenizer_trained(inputs, return_tensors=\"pt\", padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ccb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how our model performs\n",
    "inputs = \"‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ò‡§ü‡§®‡§æ ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•ã\"\n",
    "inputs_tokenized = tokenizer(inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# translation = translator(inputs, return_text=True)\n",
    "# translation = [t[\"translation_text\"] for t in translation]\n",
    "# print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b168cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_hi_to_en\", model=trainer.model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how our model performs\n",
    "inputs = \"‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ò‡§ü‡§®‡§æ ‡§ï‡•ã ‡§π‡§æ‡§á‡§≤‡§æ‡§á‡§ü ‡§ï‡§∞‡•ã\"\n",
    "\n",
    "translation = translator(inputs, return_text=True)\n",
    "translation = [t[\"translation_text\"] for t in translation]\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbe7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
