{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38aee0e",
   "metadata": {},
   "source": [
    "found here : https://github.com/ImperialNLP/NLPLabs/blob/c724834960345085690802233966682bc3321723/lab06/lab06_solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410fc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the corpus\n",
    "URL=\"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok\"\n",
    "\n",
    "#cd data\n",
    "\n",
    "for split in \"train\" \"val\" \"test_2016_flickr\"; do\n",
    "    for lang in en de fr; do\n",
    "        fname=\"${split}.lc.norm.tok.${lang}\"\n",
    "        if [ ! -f $fname ]; then\n",
    "            echo \"Downloading $fname\"\n",
    "            wget -q \"${URL}/$fname\" -O \"${split/_2016_flickr/}.${lang}\"\n",
    "        fi\n",
    "    done\n",
    "done\n",
    "echo \n",
    "\n",
    "# Print the first 10 lines with line numbers of \n",
    "# the English and French training data\n",
    "cat -n train.en | head -n10\n",
    "echo\n",
    "cat -n train.fr | head -n10\n",
    "echo\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148895e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi30K:\n",
    "    \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
    "    def __init__(self, tokenizer, src_file, trg_file):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "        self.src_sents, self.trg_sents = self.read_sentences(src_file, trg_file)\n",
    "\n",
    "    def read_sentences(self, src_file, trg_file):\n",
    "        src_sents = []\n",
    "        trg_sents = []\n",
    "\n",
    "        # Read source side\n",
    "        with open(src_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                src_sents.append(line) \n",
    "            \n",
    "        # Read target side\n",
    "        with open(trg_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                trg_sents.append(line)\n",
    "\n",
    "        assert len(src_sents) == len(trg_sents), \"Files are not aligned!\"\n",
    "        return src_sents, trg_sents\n",
    "    \n",
    "    def collate_fn(self, idx):\n",
    "        src_texts = [self.src_sents[i] for i in idx]\n",
    "        trg_texts = [self.trg_sents[i] for i in idx]\n",
    "        \n",
    "        output = self.tokenizer.prepare_seq2seq_batch(src_texts=src_texts, \n",
    "                                                      tgt_texts=trg_texts, \n",
    "                                                      max_length=128, \n",
    "                                                      max_target_length=128,\n",
    "                                                      return_tensors='pt',\n",
    "                                                      truncation=True)\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2661e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74283685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mt():\n",
    "    \n",
    "    ## QUESTION 5 ##\n",
    "\n",
    "    mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "    mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\n",
    "    mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
    "    \n",
    "    model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./experiment/mt',\n",
    "        learning_rate = 0.00005,\n",
    "        logging_steps= 5000,\n",
    "        save_steps = 10000,\n",
    "        num_train_epochs = 1,\n",
    "        per_device_train_batch_size=2\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,                         \n",
    "        args=training_args,                 \n",
    "        train_dataset=mt_dataset,                     \n",
    "        data_collator=mt_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    ## when you already trained your model and want to start from a checkpoint\n",
    "    #trainer.train(\"./experiment/mt/checkpoint-40000\")\n",
    "\n",
    "    trainer.save_model('./models/mt_marianmt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf26d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def evaluate_mt(model,mt_tokenizer, mt_test_dataset):\n",
    "\n",
    "  bleu = []\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "#   for file in tqdm(range(len(mt_test_dataset))):\n",
    "  for file in range(len(mt_test_dataset)):\n",
    "\n",
    "    src_text = mt_test_dataset.src_sents[file]\n",
    "    targ_text_origin = mt_test_dataset.trg_sents[file]\n",
    "\n",
    "    translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
    "    translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "    bleu.append(sacrebleu.corpus_bleu(translated_text, targ_text_origin, force=True).score)\n",
    "\n",
    "  bleu = np.asarray(bleu)\n",
    "\n",
    "  return np.average(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarianMTModel.from_pretrained('./models/mt_marianmt/')\n",
    "\n",
    "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
    "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
    "\n",
    "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\n",
    "\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415154e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
